---
title: "Historical MAL dataset cleanup"
author: "Edward Yu"
date: "December 07, 2018"
output:
  pdf_document:
      toc: true
      toc_depth: 2
      df_print: kable
      citation_package: natbib
bibliography: bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = TRUE,
                      error   = FALSE,
                      message = FALSE,
                      warning = FALSE)
```

# Introduction
## *From Marek*: 
### If you are interested I have a small project for R, which is very useful. It has to do with history records of MAL. Here is some basic info:
  
  * Goal: Clean up and consolidate dataset to enable easy searching of past melt
records
  * Tasks: 
    + Mostly working with strings removing duplicates ( E. Yu, YU, Yu Edward …)
    + Removing empty records
    + Missing data
    + Multiple variables in one column
  
There might be other things to do but I have not spent much time looking at the dataset. We could also pull some basic stats on usage, costs, repeats etc. I don’t know your skill level, but it is relatively simple project and I am estimating it would take me about 8 hrs of work. Actual coding, if you know what to use, could be done in less than 1 hour but that requires proficiency in typing and in R.  

I just noticed that sand for this year should have all been W410, excel incremented the name by 1 each time. I think I might be adding information about individual tests from this year incrementally as it comes in and since it is only several rows, perhaps you can delete the entire set of rows from this year, if that makes things easier on your end.

# Load & peak data
## Load packages 
```{r}
if(!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if(!require(naniar)) install.packages("naniar")
library(naniar) # gg_miss
```

## Import data
```{r}
rm(list=ls())
setwd("~/R/historical.MAL")
x <- read_csv("data/History.csv")
glimpse(x)
```

## Check levels
```{r}
# create function for later use
get_levels <- function(df, col){
  x.levels <- cbind(colnames(df),
                    (as.data.frame(sapply(df,function(x) length(unique(x)))))
  )
  colnames(x.levels) <- c("var","levels")
  row.names(x.levels) <- NULL
  levels <- x.levels[order(-x.levels[,2]),]
  return(levels[col,])
}
get_levels(x)
```

## Peak missing values
```{r}
gg_miss_var(x, show_pct = T)
gg_miss_which(x)
```

## Outline of actions to take
Rename variables to be all lowercase with no spaces. Seems the most important variables are casting type and allow type, as these are the only with zero missing values.

 * **Request**: should have 3,629 levels
 * **ID**: not utilized in recent pours, delete
 
 * **Date received**: convert to date format, fill missing values, for some reason there are less dates received than dates completed
 * **Date poured**: convert to date format, fill missing values
 * **Date completed**: convert to date format, fill missing values, perhaps create new column calculating days to complete from date received/completed
 
 * **Notes ML**: n/a
 * **Special projects**: most values are missing, unsure of importance of this field, should likely merge with comments or remove entirely  
 
 * **Requested by**: fill missing values, will require some renaming/matching
 * **Customer name**: fill missing values, will require some renaming/matching
 * **Product tested**: fill missing values, will require some renaming/matching
 
 * **Alloy**: n/a
 * **Casting type**: n/a
 * **Number of castings**: some n/a values, fill in with rounded averages 
 * **lbs**: lbs of metal used, could be calculated based on values,  fill missing values
 
 * **Sand type**: fill missing values, will require some renaming/matching
 * **Amount used**: sand? unsure what amount this is talking about
 
 * **Furnace cycle**: need to come up with new way to ID new lining and cycles

 * **Total hours**: many n/a values, should be calculated automatically based on number of castings, casting type, etc
 * **Total cost**: fill missing values, perhaps determine how it is calculated to automate the calculation
 
We have many missing datapoints, fields that aren't inuitive, some useless fields, fields that need added, etc. We'll start with the most simple and move on.  

# Cleaning
## Rename columns
Convert column names to lower case, replace spaces with periods.  
```{r}
names <- tolower(colnames(x))    # convert to lowercase
names <- gsub("  ", " ", names)  # remove double spaces
names <- gsub(" ", "\\.", names) # replace space with .
names[c(12,14)] <- c("alloy.lbs", "sand.lbs")
colnames(x) <- names
colnames(x)
```

## $request
There is a duplicate entry somewhere.
```{r}
which(duplicated(x$request)==TRUE)
as.data.frame(t(x[3609:3611,]))
```

The first entry appears to have been made in error until we see the furnace cycle was incremented. Probably shouldn't remove, will simply re-assign all request variables to equal row numbers.  
```{r}
x <- x %>% 
  mutate(request = seq(1:nrow(x)))
get_levels(x, 1)
```
 
## $id
Delete useless column.
```{r}
x <- x %>% 
  select(-id)
```

## Convert dates, add lead time
Convert char to date values.  
```{r}
x <- x %>% 
  mutate(date.poured = as.Date(x$date.poured, "%m/%d/%Y")) %>% 
  mutate(date.received = as.Date(x$date.received, "%m/%d/%Y")) %>% 
  mutate(date.completed = as.Date(x$date.completed, "%m/%d/%Y"))

summary(x[c(3,2,4)])
```

With these dates we can now determine a few useful values:

  * Preprocessing time: date poured - date recieved
  * Postprocessing time: date complete - date poured
  * Lead time: date complete - date received  
  
Of course, we need to fix the erroneous entries that are pushing our Max. date all the way up to the year 2513.

### Fix far future dates
```{r}
wrong.dates <- x %>% 
  filter(date.received  > "2020-01-01" |
         date.poured    > "2020-01-01" | 
         date.completed > "2020-01-01")
as.data.frame(wrong.dates)[,c(1,3,2,4)]

# manually fix
x$date.poured[1103]    <- as.Date("2002-04-30")
x$date.completed[1198] <- as.Date("2002-08-22")
x$date.received[1889]  <- as.Date("2005-11-17")
x$date.poured[2735]    <- as.Date("2002-06-10")
x$date.received[2740]  <- as.Date("2010-06-15")
x$date.received[3106]  <- as.Date("2012-03-20")
x$date.received[3149]  <- as.Date("2012-06-19")
x$date.received[3341]  <- as.Date("2013-11-01")
x$date.completed[3582] <- as.Date("2016-04-05")
# x[3605:3608,]
x$date.poured[3606] <- as.Date("2017-08-24")

# dates now seem to be in a normal range
summary(x[c(3,2,4)])
```

### Calculate lead times
```{r}
## create function so that results of editing can be seen quickly
calc_lead <- function(){
  preprocessing.time  <- as.numeric(x$date.poured-x$date.received)
  postprocessing.time <- as.numeric(x$date.completed-x$date.poured)
  lead.time           <- preprocessing.time + postprocessing.time
  x.temp <- as_tibble(cbind(x,
                            preprocessing.time,
                            postprocessing.time,
                            lead.time))
  return(x.temp)
}

summary(calc_lead()[c(19:21)])
```

Again, our Max. values are way off, this time our Min. values are equally bad.

### Fix large processing values
```{r}
## fix large values
wrong.dates <- calc_lead() %>% 
  filter(preprocessing.time > 400 |
           postprocessing.time > 400)
wrong.dates[c(1,3,2,4)]

# manually fix
x$date.received[310]  <- as.Date("1999-10-20")
x$date.poured[930]    <- as.Date("2001-08-30")
x$date.poured[1091]   <- as.Date("2002-04-22")
x$date.received[1616] <- as.Date("2004-04-13")
x$date.poured[1668]   <- as.Date("2004-08-04")
x$date.poured[1877]   <- as.Date("2005-11-01")
x$date.received[2229] <- as.Date("2007-01-07")
x$date.poured[2735]   <- as.Date("2010-06-10")
x$date.poured[3133]   <- as.Date("2012-05-17")

# max values look better now
summary(calc_lead()[c(19:21)])
```

### Fix negative processing values
This occurs when dates are not in proper chronology: date.received < date.poured < date.completed. We can fix this by filtering for dates that do not meet this criteria and adjusting them based on available dates and median values for preprocessing/postprocessing/lead times. 

```{r}
# fix negatives
# must follow: received < poured < completed
wrong.dates2 <- calc_lead() %>% 
  filter(preprocessing.time < 0 | postprocessing.time < 0)
wrong.dates2[c(1,3,2,4)]
# 234+ rows, definitely not doing this manually

# remove all rows listed, 
# calculate average lead times on remaining dataset,
# insert dates into removed dataset, merge 
x.anti <- anti_join(calc_lead(), wrong.dates2, c("request"))

# have NA values in date.completed
summary(wrong.dates2[c(3,2,4)])
# if NA change completed date to received + 9

# > summary(x.anti[c(19:21)])
# preprocessing.time postprocessing.time   lead.time     
# Median :  6.000    Median :  3.000     Median :  9.00  

for (i in 1:dim(wrong.dates2)[1]){
  if (is.na(wrong.dates2$date.completed[[i]]) == TRUE){
    wrong.dates2$date.completed[[i]] <- wrong.dates2$date.received[[i]] + 9
  }
}
# no more NA's
summary(wrong.dates2[c(3,2,4)])

# now we fix errors in chronology causing negative time calculations
summary(wrong.dates2[c(19:21)])

# start with received coming before poured
# wrong.dates2$date.received > wrong.dates2$date.poured
wrong.dates2[c(2,3,6,7),c(3,2,4)]

# then completed coming before poured
# wrong.dates2$date.poured > wrong.dates2$date.completed
# wrong.dates2$date.completed < wrong.dates2$date.poured
wrong.dates2[c(1,4,5,8),c(3,2,4)]

for (i in 1:dim(wrong.dates2)[1]){
  # preprocessing time = poured - received; median = 6
  if (wrong.dates2$date.received[[i]] > wrong.dates2$date.poured[[i]]){
    wrong.dates2$date.received[[i]] <- wrong.dates2$date.poured[[i]] - 6
  }
  # postprocessing time = completed - poured; median = 3
  if (wrong.dates2$date.completed[[i]] < wrong.dates2$date.poured[[i]]){
    wrong.dates2$date.completed[[i]] <- wrong.dates2$date.poured[[i]] + 3
  }
}

# confirm chronology
# wrong.dates2$date.received <= wrong.dates2$date.poured
# wrong.dates2$date.poured <= wrong.dates2$date.completed

# change date values on original dataframe and recalculate
# loop produces an error when counter > nrows, still fine though
counter=1
for (i in 1:nrow(x)){
    if (x$request[[i]] == wrong.dates2$request[[counter]]){
      x[i,c(2,3,4)] <- wrong.dates2[counter,c(2,3,4)]
      counter=counter+1
    }
}

# date summary looks okay now... except for NA's
summary(calc_lead()[c(19:21)])
```

### Fix NA values in dates
Our calculations have NA values which means our dates must have NA's. We'll check which date columns contain NA's and in a similar fashion to above will extrapolate appropriate times based on the calculated median values above.  
```{r}
summary(x)[,c(3,2,4)]

# fill NA values using date.poured to calculate received and completed
# guess on single NA
x %>% filter(is.na(date.poured))
(x[3608:3612,])
(x$date.poured[3611] - x$date.poured[3609]) / 2 # 144 days between dates
# just assign the middle date
x$date.poured[3610] <- x$date.poured[3609] - 72

# now date.poured has no NA's and we can extrapolate from this
summary(x)[,c(3,2,4)]

x.rec <- x %>% 
  filter(is.na(date.received)) %>% 
  mutate(date.received = date.poured - 6)

counter=1
for (i in 1:nrow(x)){
  if (x$request[[i]] == x.rec$request[[counter]]){
    x[i,c(2,3,4)] <- x.rec[counter,c(2,3,4)]
    counter=counter+1
  }
}

x.com <- x %>% 
  filter(is.na(date.completed)) %>% 
  mutate(date.completed = date.poured + 3)

counter=1
for (i in 1:nrow(x)){
  if (x$request[[i]] == x.com$request[[counter]]){
    x[i,c(2,3,4)] <- x.com[counter,c(2,3,4)]
    counter=counter+1
  }
}

# NOW we have zero NA values
summary(calc_lead()[c(19:21)])

# assign our new values to the df
x <- calc_lead()
```

## $special.projects
Seems to be a redundant column when the $notes column would suffice. Check if values are stored in the column and concatenate them with the notes column.  
```{r}
# list non-NA values in special projects
x$special.projects[!is.na(x$special.projects)]
# find rownums of non-NA vlaues
spec.rows <- which(!is.na(x$special.projects)==T)
# check notes.ml of same rownums
x$notes.ml[spec.rows]
# concatenate the columns
x[spec.rows,] <- x[spec.rows,] %>%
  mutate(notes.ml = paste(notes.ml, special.projects, sep="--"))
# confirm
x$notes.ml[spec.rows]
```

