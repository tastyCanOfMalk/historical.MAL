---
title: "Historical MAL - Cleaning Data"
author: "Edward Yu"
date: "March 6, 2019"
output:
  html_document:
      theme: flatly
      highlight: tango
      toc: true
      toc_float:
        collapsed: false
      toc_depth: 3
      df_print: paged
      code_folding: show
      # fig_width: 7
      # fig_height: 6
      # fig_caption: true
      # citation_package: natbib
# bibliography: bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = TRUE,
                      error   = FALSE,
                      message = FALSE,
                      warning = FALSE)

if(!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if(!require(gridExtra)) install.packages("gridExtra")
library(gridExtra)
if(!require(naniar)) install.packages("naniar")
library(naniar) # gg_miss
# if(!require(stringdist)) install.packages("stringdist")
# library(stringdist) # soundex?
# if(!require(phonics)) install.packages("phonics")
# library(phonics) # soundex?
if(!require(lexicon)) install.packages("lexicon")
library(lexicon) # common_names

# clear environment
rm(list=ls())

# IMPORT DATA
setwd("~/R/historical.MAL")
# setwd("I:/Code-CAD/R/historical.MAL")
```

# Introduction
If you are interested I have a small project for R, which is very useful. It has to do with history records of MAL. Here is some basic info:
  
  * Goal: Clean up and consolidate dataset to enable easy searching of past melt
records
  * Tasks: 
    + Mostly working with strings removing duplicates ( E. Yu, YU, Yu Edward …)
    + Removing empty records
    + Missing data
    + Multiple variables in one column
  
There might be other things to do but I have not spent much time looking at the dataset. We could also pull some basic stats on usage, costs, repeats etc. I don’t know your skill level, but it is relatively simple project and I am estimating it would take me about 8 hrs of work. Actual coding, if you know what to use, could be done in less than 1 hour but that requires proficiency in typing and in R.  

I just noticed that sand for this year should have all been W410, excel incremented the name by 1 each time. I think I might be adding information about individual tests from this year incrementally as it comes in and since it is only several rows, perhaps you can delete the entire set of rows from this year, if that makes things easier on your end.

# Cleaning
This is a cleaning project. We need to:
  
  1. clean 
  2. clean 
  3. clean 
  4. clean 
  5. clean...

## Preliminary exploration
There are a few key steps that are important before manipulating our data:

  * Check the structure of our data
  * Check the unique levels of our variables
  * Determine which variables are missing data


### Import
There are a few things we see right off the bat.

  * Redundant `ID` variable
  * Poorly named variables (*e.g.* `lbs`, `Amount used`)
  * Dates incorrectly formatted
  * `Special Projects` potentially useless
  
```{r}
x <- read_csv("data/History.csv")
glimpse(x)
```

### Get variable levels
Sorting by unique variable levels will give us an idea of how many unique datapoints we have in each variable category. We can make a few conclusions based on this information.

  * `Request` is apprently doing the job of `ID`
  * We likely do not have 273 unique types of castings, as suggested by `Casting type`
  * We likely do not pour using 60 different alloys, as suggested by `Alloy`
  * We see only 19 unique variables in `Special Projects`

```{r}
# FUNCTION to get levels by column
get_levels <- function(df, col){
  x.levels <- cbind(colnames(df),
                    (as.data.frame(sapply(df,function(x) length(unique(x)))))
  )
  colnames(x.levels) <- c("var","levels")
  row.names(x.levels) <- NULL
  levels <- x.levels[order(-x.levels[,2]),]
  return(levels[col,])
}

get_levels(x)
```

### Visualize missing data
The naniar package makes visualization of missing data quite simple. Some of our previous observations come up again when looking at the plots.

  * `Special Projects` is highly unused
  * `ID` is highly unused
  * Most variables are missing data

```{r}
# MISSING DATA
g1 <- gg_miss_var(x, show_pct = T)
g2 <- gg_miss_which(x)
grid.arrange(g1,g2,nrow=2)
```

### Rename variables
It's best to convert all variable names to lowercase, remove spaces, and clarify the variable names.  

```{r}
# RENAME VARIABLES
names <- tolower(colnames(x))
names <- gsub("  ", " ", names) 
names <- gsub(" ", "\\.", names) 
names[c(12,14)] <- c("alloy.lbs", "sand.lbs")
colnames(x) <- names
colnames(x)
```

## Diving in
We look at each variable, specifically, and determine the best way of handling the data.  

### Request
The `request` variable should be a unique, incremented identifier. This sounds like what the `id` variable was trying to accomplish. We'll convert `request` values to row number values while also removing the `id` variable.  

```{r}
# make request = row number, remove id
x <- x %>% 
  mutate(request = seq(1:nrow(x))) %>% 
  select(-id)
```

### Dates
As we'll see, many of the dates are not chronologically accurate. For example, `date.poured` may come before `date.received`, `date.completed` may even come before `date.received`. To correct these cases we want to detect any situation which does not follow the pattern of `date.received` < `date.poured` < `date.completed`, and correct them.

  1. Convert to `Date` format
  2. Find & correct extreme outliers
  3. Create new variables to further find outliers
  
```{r}
# convert to dates
x <- x %>% 
  mutate(date.received  = as.Date(x$date.received,  "%m/%d/%Y")) %>% 
  mutate(date.poured    = as.Date(x$date.poured,    "%m/%d/%Y")) %>% 
  mutate(date.completed = as.Date(x$date.completed, "%m/%d/%Y"))

# check summary
summary(x[c(3,2,4)])

# filter for anything too far in the future
wrong.dates <- x %>% 
  filter(date.received  > "2020-01-01" |
         date.poured    > "2020-01-01" | 
         date.completed > "2020-01-01")

as.data.frame(wrong.dates)[,c(1,3,2,4)]

# manually fix
x$date.poured[1103]    <- as.Date("2002-04-30")
x$date.completed[1198] <- as.Date("2002-08-22")
x$date.received[1889]  <- as.Date("2005-11-17")
x$date.poured[2735]    <- as.Date("2002-06-10")
x$date.received[2740]  <- as.Date("2010-06-15")
x$date.received[3106]  <- as.Date("2012-03-20")
x$date.received[3149]  <- as.Date("2012-06-19")
x$date.received[3341]  <- as.Date("2013-11-01")
x$date.completed[3582] <- as.Date("2016-04-05")
x$date.poured[3606]    <- as.Date("2017-08-24")

# dates now seem to be in a normal range
summary(x[c(3,2,4)])
```

We've fixed the obvious outliers, we need to fix chronological errors now. Easiest way to do this is to come up with aa few new metrics:

  * *Pre-processing time* days between `date.received` and `date.poured`
  * *Post-processing time* days between `date.poured` and `date.completed`
  * *Lead time* days between `date.received` and `date.completed`
  
Any abnormal values resulting from calculation of these variables will tell us that something at least one variable is not following: `date.received` < `date.poured` < `date.completed`.  

```{r}
# function to calculate times based on date values
## preprocessing  = time between receiving and pouring
## postprocessing = time between pouring and completed
## lead time      = pre + post-processing
calc_lead <- function(){
  preprocessing.time  <- as.numeric(x$date.poured-x$date.received)
  postprocessing.time <- as.numeric(x$date.completed-x$date.poured)
  lead.time           <- preprocessing.time + postprocessing.time
  x.temp <- as_tibble(cbind(x,
                            preprocessing.time,
                            postprocessing.time,
                            lead.time))
  return(x.temp)
}

summary(calc_lead()[c(19:21)])

# list large positive values
wrong.dates <- calc_lead() %>% 
  filter(preprocessing.time  > 400 |
         postprocessing.time > 400)
wrong.dates[c(1,3,2,4)]

# manually fix
x$date.received[310]  <- as.Date("1999-10-20")
x$date.poured[930]    <- as.Date("2001-08-30")
x$date.poured[1091]   <- as.Date("2002-04-22")
x$date.received[1616] <- as.Date("2004-04-13")
x$date.poured[1668]   <- as.Date("2004-08-04")
x$date.poured[1877]   <- as.Date("2005-11-01")
x$date.received[2229] <- as.Date("2007-01-07")
x$date.poured[2735]   <- as.Date("2010-06-10")
x$date.poured[3133]   <- as.Date("2012-05-17")

# max values look better now
summary(calc_lead()[c(19:21)])

# fix large negative values
wrong.dates <- calc_lead() %>% 
  filter(preprocessing.time  < 0 | 
         postprocessing.time < 0)

wrong.dates[c(1,3,2,4)]
```

We have a far larger amount of errors resulting in larger negative `lead.time`, etc, values (244), far too many to fix manually. This time, instead of fixing manually, we will extract these incorrect values from the original dataset, calculate average lead times on the remaining data, take the median value, and back-calculate the correct dates based on these measures.  

```{r}
# extract all rows listed
# calculate average lead times on remaining dataset,
# insert dates into removed dataset, merge 

x.anti <- anti_join(calc_lead(), wrong.dates, c("request"))

# anti looks good
summary(x.anti[c(19:21)])
# wrongs look bad
summary(wrong.dates[c(19:21)])

# see some NA values from the calculations
# backtrace to check dates
summary(wrong.dates[c(3,2,4)])
# have NA values in date.completed
# if NA change completed date to received + 9

# > summary(x.anti[c(19:21)])
# preprocessing.time postprocessing.time   lead.time     
# Median :  6.000    Median :  3.000     Median :  9.00  

for (i in 1:dim(wrong.dates)[1]){
  if (is.na(wrong.dates$date.completed[[i]]) == TRUE){
    wrong.dates$date.completed[[i]] <- wrong.dates$date.received[[i]] + 9
  }
}
# no more NA's
summary(wrong.dates[c(3,2,4)])

# now we fix errors in chronology causing negative time calculations
summary(wrong.dates[c(19:21)])

# start with received coming before poured
wrong.dates$date.received > wrong.dates$date.poured
wrong.dates[c(2,3,6,7),c(3,2,4)]

# then completed coming before poured
wrong.dates$date.poured > wrong.dates$date.completed
wrong.dates$date.completed < wrong.dates$date.poured
wrong.dates[c(1,4,5,8),c(3,2,4)]

for (i in 1:dim(wrong.dates)[1]){
  # preprocessing time = poured - received; median = 6
  if (wrong.dates$date.received[[i]] > wrong.dates$date.poured[[i]]){
    wrong.dates$date.received[[i]] <- wrong.dates$date.poured[[i]] - 6
  }
  # postprocessing time = completed - poured; median = 3
  if (wrong.dates$date.completed[[i]] < wrong.dates$date.poured[[i]]){
    wrong.dates$date.completed[[i]] <- wrong.dates$date.poured[[i]] + 3
  }
}

# confirm chronology
wrong.dates$date.received <= wrong.dates$date.poured
wrong.dates$date.poured <= wrong.dates$date.completed

# wrong.dates df has correct date values, need to join to original
x[c(13,40,41,191),]
wrong.dates[c(1:4),]

# test loop
# test.df <- tibble("row"=numeric())
# counter=1
# for (i in 1:nrow(x)){
#   if (x$request[[i]] == wrong.dates$request[[counter]] && counter<245){
#     test.df[counter, 1] <- x$request[[i]]
#     counter=counter+1
#   }
# }
# another test loop
# y <- x
# counter=1
# for (i in 1:nrow(x)){
#     if (y$request[[i]] == wrong.dates$request[[counter]] && counter < 244){
#       y[i,c(2,3,4)] <- wrong.dates[counter,c(2,3,4)]
#       counter=counter+1
#     }
# }
# x[c(13,40,41,191),]
# wrong.dates[c(1:4),]
# y[c(13,40,41,191),]

# change date values on original dataframe and recalculate
# loop produces an error when counter > nrows, still fine though
counter=1
for (i in 1:nrow(x)){
  if (counter == nrow(wrong.dates)+1){break}
  if (x$request[[i]] == wrong.dates$request[[counter]]){
    x[i,c(2,3,4)] <- wrong.dates[counter,c(2,3,4)]
    counter=counter+1
  }
}
# confirm
x[c(13,40,41,191,3573),]
wrong.dates[c(1:4,244),]

# date summary looks okay now... except for NA's
summary(calc_lead()[c(19:21)])

summary(x)[,c(3,2,4)]

# fill NA values using date.poured to calculate received and completed
# guess on single NA
x %>% filter(is.na(date.poured))
(x[3608:3612,])
(x$date.poured[3611] - x$date.poured[3609]) / 2 # 144 days between dates
# just assign the middle date
x$date.poured[3610] <- x$date.poured[3609] - 72

# now date.poured has no NA's
summary(x)[,c(3,2,4)]

x.rec <- x %>% 
  filter(is.na(date.received)) %>% 
  mutate(date.received = date.poured - 6)

counter=1
for (i in 1:nrow(x)){
  if (x$request[[i]] == x.rec$request[[counter]]){
    x[i,c(2,3,4)] <- x.rec[counter,c(2,3,4)]
    counter=counter+1
  }
}

x.com <- x %>% 
  filter(is.na(date.completed)) %>% 
  mutate(date.completed = date.poured + 3)

counter=1
for (i in 1:nrow(x)){
  if (x$request[[i]] == x.com$request[[counter]]){
    x[i,c(2,3,4)] <- x.com[counter,c(2,3,4)]
    counter=counter+1
  }
}

# NOW we have zero NA values
summary(calc_lead()[c(19:21)])

# assign our new values
x <- calc_lead()

```



request
id
date.poured
date.received
date.completed
requested.by
customer.name
product.tested
casting.type
number.of.castings
alloy
alloy.lbs
sand.type
sand.lbs
total.hours
total.cost
furnace.cycle
notes.ml
special.projects

# REQUEST



###########################
# ID
x <- x %>% 
  select(-id)
















### Test tabbed table {.tabset}

#### By Product
stuf herekfajhdlkahlkjflnkja
ds
dsfdsfsgfdgfgsdf

#### By Region
(tab content)kdsjfnkjdfsbjgfvkzkf

